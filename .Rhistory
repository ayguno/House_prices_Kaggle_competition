prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions = training.portions[j]
}
}
training.portions <- c(0.5,0.7,0.8)
results <- data.frame(training.portions = 1:300, error = 1:300)
for(j in seq_along(training.portions)){
set.seed(1)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions = training.portions[j]
}
}
View(results)
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
for (i in 1:100){
results <- data.frame(training.portions = 1:100, error = 1:100)
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
}
final.results <- rbind(final.results,results)
}
View(final.results)
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
}
final.results <- rbind(final.results,results)
}
View(final.results)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw() +scale_color_discrete()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter(color = c("navy","red","purple")) +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error),color = c("navy","red","purple"), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error),fill = c("navy","red","purple"), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error) data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()  +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
library(dplyr)
library(dplyr)
final.results %>% group_by(student.included,training.portions) %>% summarise(mean.error = mean(error))
library(dplyr)
final.results %>% group_by(student.included,training.portions) %>% summarise(mean.error = mean(error), sd.error = sd(error))
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error, group = training.portions), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error, group = training.portions), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete()
ggplot(aes(x =  error, group = student.included , color = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete()
ggplot(aes(x =  error, group = student.included , color = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete() +facet_grid(. ~ training.portions)
install.packages("glmnet")
library(glmnet)
library(ISLR)
data("Hitters")
Hitters
y = model.matrix(Salary ~ . -1, data = Hitters)
View(y)
View(y)
x = model.matrix(Salary ~ . -1, data = Hitters)
y = Hitters$Salary
Hitters <- na.omit(Hitters)
View(x)
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# We create a response vector
y = Hitters$Salary
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . , data = Hitters)
# We create a response vector
y = Hitters$Salary
View(x)
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# We create a response vector
y = Hitters$Salary
View(x)
View(Hitters)
View(x)
data.frame(names(Hitters),names(x))
data.frame(names(Hitters),col.names(x))
data.frame(names(Hitters),colnames(x))
fit.ridge <- glmnet(x, y, alpha = 0)
summary(fit.ridge)
coef(fit.ridge)
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
cv.ridge <- cv.glmnet(x,y,alpha = 0)
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
fit.lasso <- glmnet(x,y)
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
cv.lasso$glmnet.fit$call
summary(cv.lasso$glmnet.fit)
cv.lasso$name
cv.lasso$nzero
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
# Chunk 2
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables
# We create a response vector
y = Hitters$Salary
# Chunk 3
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
# Chunk 4
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
# Chunk 5
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
# Chunk 6
plot(fit.lasso, xvar = "dev", label = TRUE)
# Chunk 7
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
coef(fit.lasso)
coef(cv.lasso)
pred <- predict(fit.lasso,x)
dim(pred)
fit.lasso
View(pred)
a = y -pred
View(a)
View(pred)
dim(y)
length(y)
rmse <- sqrt(apply((y-pred)^2,2,mean))
rmse <- sqrt(apply((y-pred)^2,2,mean))
plot(log(fit.lasso$lambda),rmse, type = "b", col = "navy")
rmse[1]
rmse[2]
order(rmse)[1]
order(rmse)[1]
fit.lasso$lambda
fit.lasso$lambda[order(rmse)]
fit.lasso$lambda[order(rmse)[1]]
fit.lasso$lambda[order(rmse)][1]
best.lambda <- fit.lasso$lambda[order(rmse)][1]
best.lambda
coef(fit.lasso, s = best.lambda)
cv.lasso
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s= "lambda.1se")
View(cv.pred)
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s= "lambda.1se")
rmse.cv = sqrt((y-cv.pred) ^2)
rmse.cv
rmse.cv = sqrt(mean((y-cv.pred) ^2))
coef(cv.lasso)
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s = "lambda.1se")
rmse.cv = sqrt(mean((y-cv.pred) ^2))
rmse.cv
install.packages("pls")
library(pls)
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type = "RMSEP")
validationplot(pcr.fit,val.type = "MSEP")
validationplot(pcr.fit,val.type = "R2")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
# Chunk 2
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables
# We create a response vector
y = Hitters$Salary
# Chunk 3
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
# Chunk 4
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
# Chunk 5
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
# Chunk 6
plot(fit.lasso, xvar = "dev", label = TRUE)
# Chunk 7
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
# Chunk 8
coef(cv.lasso)
# Chunk 9
fit.lasso
pred <- predict(fit.lasso,x)
dim(pred)
length(y)
# Chunk 10
rmse <- sqrt(apply((y-pred)^2,2,mean))
plot(log(fit.lasso$lambda),rmse, type = "b", col = "navy")
# The smallest test rmse we obtained:
order(rmse)[1]
# Learn the best lambda: the lambda that yields the model that gives the minimum rmse:
best.lambda <- fit.lasso$lambda[order(rmse)][1]
best.lambda
# Get the model coefficients that are found by the best lambda:
# Note the s = argument refers to lambda
coef(fit.lasso, s = best.lambda)
# Chunk 11
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s = "lambda.1se")
rmse.cv = sqrt(mean((y-cv.pred) ^2))
rmse.cv
# Chunk 12
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV")
# Chunk 13
summary(pcr.fit)
# Chunk 14
validationplot(pcr.fit,val.type = "RMSEP")
validationplot(pcr.fit,val.type = "MSEP")
validationplot(pcr.fit, val.type = "R2")
pcr.pred <- predict(pcr.fit,newdata = Hitters,ncomp = 5)
pcr.pred
mean( (y - pcr.pred)^ 2)
rmse.cv ^2
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = " CV" )
summary(pls.fit)
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV" )
summary(pls.fit)
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV" )
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
pls.pred <- predict(pls.fit, newdata = Hitters, ncomp = 5)
mean((y - pls.pred)^2)
pls.pred <- predict(pls.fit, newdata = Hitters, ncomp = 5)
# Calculate MSE:
mean((y - pls.pred)^2)
# compare MSEs for all models we used:
MSE.table <- data.frame(lasso = (rmse.cv ^2),
pcr = mean((y- pcr.pred)^2),
pls = mean((y- pls.pred)^2))
MSE.table
APPNAME <<- sub('.*/','',getwd())
getwd()
set.seed(123)
rnorm(1,2)
for(i in 1:23){}
for(i in 1:23){print(i)}
lapply(1:23, print)
sapply(1:23,print)
x<-sapply(1:23, sqrt)
x
setwd("~/Desktop/2016/Data_science/Kaggle/House_prices_Kaggle_competition")
knitr::opts_chunk$set(results = "markup", fig.align = "center",
fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
train <- read.csv("train.csv"); test <- read.csv("test.csv")
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
mostly_missing <- which(na.sum > (0.3 * nrow(train)))
na.sum[mostly_missing]
View(train)
train$Alley[1]
as.character(train$Alley[1])
as.character(train$Alley[1])
as.character(train$Alley[1])
is.na(as.character(train$Alley[1]))
train$Alley[which(is.na(as.character(train$Alley)))] <- "NoAlley"
train$Alley[which(is.na(as.character(train$Alley)))] <- "NoAlley"
which(is.na(as.character(train$Alley)))
as.character(train$Alley[which(is.na(as.character(train$Alley)))]) <- "NoAlley"
train$Alley <- as.character(train$Alley)
train$Alley[which(is.na(train$Alley))] <- "NoAlley"
train$Alley <- factor(train$Alley)
View(train)
library(ggplot2)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()
test$Alley <- as.character(test$Alley)
test$Alley[which(is.na(test$Alley))] <- "NoAlley"
test$Alley <- factor(test$Alley)
ggplot(data = train, aes(x = FireplaceQu, y = Salesprice, fill= FireplaceQu))+
geom_boxplot()
ggplot(data = train, aes(x = FireplaceQu, y = log10(SalePrice), fill= FireplaceQu))+
geom_boxplot()
hist(train$SalePrice)
hist(train$SalePrice)
hist(log10(train$SalePrice))
FirePlaceFit <- lm(log10(train$SalePrice) ~ FireplaceQu, data = train)
summary(FirePlaceFit)
plot(FirePlaceFit)
par(mfrow=c(1,2))
hist(train$SalePrice)
hist(log10(train$SalePrice)) # Looks more gaussian
FirePlaceFit <- lm(log10(train$SalePrice) ~ FireplaceQu, data = train)
summary(FirePlaceFit)
par(mfrow=c(2,2))
plot(FirePlaceFit)
train$FireplaceQu <- as.character(train$FireplaceQu)
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "NoFireplace"
train$FireplaceQu <- factor(train$FireplaceQu)
#Also transform the test set
test$FireplaceQu <- as.character(test$FireplaceQu)
test$FireplaceQu[which(is.na(test$FireplaceQu))] <- "NoFireplace"
test$FireplaceQu <- factor(test$FireplaceQu)
View(test)
ggplot(data = train, aes(x = FireplaceQu, y = log10(SalePrice), fill= FireplaceQu))+
geom_boxplot()
