for (i in 1:100){
results <- data.frame(training.portions = 1:100, error = 1:100)
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
}
final.results <- rbind(final.results,results)
}
View(final.results)
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
}
final.results <- rbind(final.results,results)
}
View(final.results)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw() +scale_color_discrete()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter(color = c("navy","red","purple")) +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error),color = c("navy","red","purple"), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error),fill = c("navy","red","purple"), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error) data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) + geom_jitter() +geom_boxplot() +theme_bw()
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()  +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
training.portions <- c(0.5,0.7,0.8)
final.results <- NULL
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "No"
}
final.results <- rbind(final.results,results)
}
# Model including the student dummy variable
for(j in seq_along(training.portions)){
set.seed(1)
results <- data.frame(training.portions = 1:100, error = 1:100,
student.included = 1:100)
for (i in 1:100){
Intrain <- createDataPartition(Default$default, p = 0.7, list = FALSE)
train.set <- Default[Intrain,]
test.set <- Default[-Intrain,]
glm.train <- glm(default ~ income + balance + student.binary, data = train.set, family = "binomial")
# Vector of the fitted probabilities
prob.fitted <- predict(glm.train,test.set)
# Classifications
temp <- ifelse(prob.fitted > 0.5,"Yes", "No")
results$error[i] <- mean(temp != test.set$default)
results$training.portions[i] = training.portions[j]
results$student.included[i] = "Yes"
}
final.results <- rbind(final.results,results)
}
library(ggplot2)
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_boxplot()+ geom_jitter()+ facet_grid( . ~ student.included) +theme_bw()
library(dplyr)
library(dplyr)
final.results %>% group_by(student.included,training.portions) %>% summarise(mean.error = mean(error))
library(dplyr)
final.results %>% group_by(student.included,training.portions) %>% summarise(mean.error = mean(error), sd.error = sd(error))
ggplot(aes(x = factor(training.portions), y = error), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error, group = training.portions), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw()
ggplot(aes(x =  error, group = training.portions), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density()+ facet_grid( . ~ student.included) +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_hue()
ggplot(aes(x =  error, group = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete()
ggplot(aes(x =  error, group = student.included , color = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete()
ggplot(aes(x =  error, group = student.included , color = student.included), data = final.results) +geom_density() +theme_bw() +scale_colour_discrete() +facet_grid(. ~ training.portions)
install.packages("glmnet")
library(glmnet)
library(ISLR)
data("Hitters")
Hitters
y = model.matrix(Salary ~ . -1, data = Hitters)
View(y)
View(y)
x = model.matrix(Salary ~ . -1, data = Hitters)
y = Hitters$Salary
Hitters <- na.omit(Hitters)
View(x)
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# We create a response vector
y = Hitters$Salary
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . , data = Hitters)
# We create a response vector
y = Hitters$Salary
View(x)
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# We create a response vector
y = Hitters$Salary
View(x)
View(Hitters)
View(x)
data.frame(names(Hitters),names(x))
data.frame(names(Hitters),col.names(x))
data.frame(names(Hitters),colnames(x))
fit.ridge <- glmnet(x, y, alpha = 0)
summary(fit.ridge)
coef(fit.ridge)
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
cv.ridge <- cv.glmnet(x,y,alpha = 0)
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
fit.lasso <- glmnet(x,y)
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
cv.lasso$glmnet.fit$call
summary(cv.lasso$glmnet.fit)
cv.lasso$name
cv.lasso$nzero
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
# Chunk 2
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables
# We create a response vector
y = Hitters$Salary
# Chunk 3
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
# Chunk 4
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
# Chunk 5
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
# Chunk 6
plot(fit.lasso, xvar = "dev", label = TRUE)
# Chunk 7
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
coef(fit.lasso)
coef(cv.lasso)
pred <- predict(fit.lasso,x)
dim(pred)
fit.lasso
View(pred)
a = y -pred
View(a)
View(pred)
dim(y)
length(y)
rmse <- sqrt(apply((y-pred)^2,2,mean))
rmse <- sqrt(apply((y-pred)^2,2,mean))
plot(log(fit.lasso$lambda),rmse, type = "b", col = "navy")
rmse[1]
rmse[2]
order(rmse)[1]
order(rmse)[1]
fit.lasso$lambda
fit.lasso$lambda[order(rmse)]
fit.lasso$lambda[order(rmse)[1]]
fit.lasso$lambda[order(rmse)][1]
best.lambda <- fit.lasso$lambda[order(rmse)][1]
best.lambda
coef(fit.lasso, s = best.lambda)
cv.lasso
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s= "lambda.1se")
View(cv.pred)
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s= "lambda.1se")
rmse.cv = sqrt((y-cv.pred) ^2)
rmse.cv
rmse.cv = sqrt(mean((y-cv.pred) ^2))
coef(cv.lasso)
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s = "lambda.1se")
rmse.cv = sqrt(mean((y-cv.pred) ^2))
rmse.cv
install.packages("pls")
library(pls)
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV")
summary(pcr.fit)
validationplot(pcr.fit,val.type = "RMSEP")
validationplot(pcr.fit,val.type = "MSEP")
validationplot(pcr.fit,val.type = "R2")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE,results = "markup", fig.align = "center", fig.width = 6, fig.height = 6,message=FALSE,warning=FALSE)
# Chunk 2
library(glmnet)
library(ISLR)
data("Hitters")
# We will remove missing values for this example
Hitters <- na.omit(Hitters)
# We create a matrix of predictors
x = model.matrix(Salary ~ . -1, data = Hitters)
# Note we use all predictors Salary ~ .
# Note we add -1 since we don't want an intercept to appear
# Note model.matrix drops the outcome vector, and it converts the factor variables into dummy variables
# We create a response vector
y = Hitters$Salary
# Chunk 3
fit.ridge <- glmnet(x, y, alpha = 0)
plot(fit.ridge,xvar = "lambda", label = TRUE)
# Chunk 4
# Note that default is 10-fold cross-validation
cv.ridge <- cv.glmnet(x,y,alpha = 0)
plot(cv.ridge)
# Chunk 5
# Fit the lasso regression model
fit.lasso <- glmnet(x,y)
plot(fit.lasso, xvar = "lambda", label = TRUE)
# Chunk 6
plot(fit.lasso, xvar = "dev", label = TRUE)
# Chunk 7
cv.lasso <- cv.glmnet(x,y)
plot(cv.lasso)
# Chunk 8
coef(cv.lasso)
# Chunk 9
fit.lasso
pred <- predict(fit.lasso,x)
dim(pred)
length(y)
# Chunk 10
rmse <- sqrt(apply((y-pred)^2,2,mean))
plot(log(fit.lasso$lambda),rmse, type = "b", col = "navy")
# The smallest test rmse we obtained:
order(rmse)[1]
# Learn the best lambda: the lambda that yields the model that gives the minimum rmse:
best.lambda <- fit.lasso$lambda[order(rmse)][1]
best.lambda
# Get the model coefficients that are found by the best lambda:
# Note the s = argument refers to lambda
coef(fit.lasso, s = best.lambda)
# Chunk 11
cv.pred<- predict.cv.glmnet(cv.lasso,newx = x, s = "lambda.1se")
rmse.cv = sqrt(mean((y-cv.pred) ^2))
rmse.cv
# Chunk 12
library(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV")
# Chunk 13
summary(pcr.fit)
# Chunk 14
validationplot(pcr.fit,val.type = "RMSEP")
validationplot(pcr.fit,val.type = "MSEP")
validationplot(pcr.fit, val.type = "R2")
pcr.pred <- predict(pcr.fit,newdata = Hitters,ncomp = 5)
pcr.pred
mean( (y - pcr.pred)^ 2)
rmse.cv ^2
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = " CV" )
summary(pls.fit)
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV" )
summary(pls.fit)
set.seed(1)
pls.fit <- plsr(Salary ~ . , data = Hitters, scale = TRUE, validation = "CV" )
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
pls.pred <- predict(pls.fit, newdata = Hitters, ncomp = 5)
mean((y - pls.pred)^2)
pls.pred <- predict(pls.fit, newdata = Hitters, ncomp = 5)
# Calculate MSE:
mean((y - pls.pred)^2)
# compare MSEs for all models we used:
MSE.table <- data.frame(lasso = (rmse.cv ^2),
pcr = mean((y- pcr.pred)^2),
pls = mean((y- pls.pred)^2))
MSE.table
APPNAME <<- sub('.*/','',getwd())
getwd()
set.seed(123)
rnorm(1,2)
for(i in 1:23){}
for(i in 1:23){print(i)}
lapply(1:23, print)
sapply(1:23,print)
x<-sapply(1:23, sqrt)
x
setwd("~/Desktop/2016/Data_science/Kaggle/House_prices_Kaggle_competition")
#############
# Getting the data files
#############
setwd("~/Desktop/2016/Data_science/Kaggle/House_prices_Kaggle_competition")
testURL <- "https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/test.csv?GoogleAccessId=competitions-data@kaggle-161607.iam.gserviceaccount.com&Expires=1512002091&Signature=Iyzcu0kfgpwTx2oO0GMUyfGbPp5WM7S13a%2Bbt7SdJNTXo%2FOtpQgoW9k%2BrV65bjG6vhNIohOVXYw10JzvoDeHw4w07Ymuwyv0Qt5aU8KAxV4ZoJ2Gc3uvems%2Bw6WCe4xCFp5o8m20NtTlLdccMNWXe%2FGGFFcTNYFpFvx9G5dmeTwjlaBOsyDdm5onF4G2PG4Qi5GJfuZ213808IofjxArwMEyt4aWiP5%2B0f26DiZxUoNniPmUESrkGg47b78dPL9ip6%2FpnTZJsgPnk8KI8KQyq%2BgzZRTJdbwCGiopDiGz9VQERCKHig4W3eniPYtdByXbtT8myVRMdU%2BW7hw0WgTwkg%3D%3D"
download.file(testURL, destfile = "./test.csv")
trainURL <-"https://storage.googleapis.com/kaggle-competitions-data/kaggle/5407/train.csv?GoogleAccessId=competitions-data@kaggle-161607.iam.gserviceaccount.com&Expires=1512002321&Signature=BXfAxWBLVg%2BCnlx70kkKU7FBqLrB0mBXkvSCxMN10yORFGjHxWnTvXl%2FDCc%2Bcx10URdODg%2FW0s6RdGLNxrOyImlAMTSRNBk1jZJWCsv%2BY0TfRIKjb0R9ebD44m1b07vTDIdIE%2BnF2x4zBLrwR%2FMdCLOA9VQaf0uGYaPKD%2BewwzsU5i3sTr0VK03SBn6NUlst9W8g69irdAoNqjuDHhCHTwyQ4dQ6JxNVMeq9ZOL33XlXxuhfVaUvL1bwSUc8OcW%2BBCKlFPfzfjR8AKDx4D5fAS2Yn6cmivXHG0xJswSkdqRvHiDlHpUaAIEh4RzmF5rozEOZ1Btf6TIs5Ka9atirYg%3D%3D"
download.file(trainURL, destfile = "./train.csv")
train <- read.csv("train.csv")
View(train)
train <- read.csv("train.csv"); test <- read.csv("test.csv")
summary(train)
boxplot(train$SalePrice)
na.status <- is.na(train)
View(na.status)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.sum
mostly_missing <- which(na.sum > (0.3 * nrow(train)))
na.sum[mostly_missing]
library(ggplot2)
ggplot(data = train, aes(x = Alley, y = SalePrice))+
geom_boxplot()
ggplot(data = train, aes(x = Alley, y = log(SalePrice+1)))+
geom_boxplot()
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()+
scale_fill_brewer(palette = 3)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()+
scale_fill_brewer(palette = 2)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()+
scale_fill_brewer(palette = 1)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()+
scale_fill_discrete(palette = 1)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()+
scale_fill_discrete()
knitr::opts_chunk$set(results = "markup", fig.align = "center",
fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
summary(train)
boxplot(log(train$SalePrice))
train <- read.csv("train.csv"); test <- read.csv("test.csv")
summary(train)
boxplot(log(train$SalePrice))
boxplot(log10(train$SalePrice))
par(mfrow = c(1,2))
par(mfrow = c(1,2))
par(mfrow = c(1,2))
boxplot(train$SalePrice,main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
par(mfrow = c(1,2))
boxplot(train$SalePrice,main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
mostly_missing <- which(na.sum > (0.3 * nrow(train)))
na.sum[mostly_missing]
library(ggplot2)
ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
geom_boxplot()
