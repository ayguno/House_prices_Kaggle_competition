---
title: "Housing Prices"
author: "Ozan Aygun"
date: "11/26/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary

# Loading the data


```{r}
train <- read.csv("train.csv"); test <- read.csv("test.csv")
```

# Summarizing the data
```{r}
summary(train)
```

The outcome variable SalePrice is a complete variable, but there are a number of predictors with many missing values. Median house price is $163,000.

```{r}
par(mfrow = c(1,2))
boxplot(train$SalePrice,main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")
```

## Understanding the impact of missing data

Which features have more than 30% missing data?

```{r}
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
mostly_missing <- which(na.sum > (0.3 * nrow(train)))
na.sum[mostly_missing]
```

5 variables have substantial amount of missing data. Do they have impact on the house prices?

###Alley

Based on the data description: Type of alley access to property

       Grvl     Gravel
       Pave     Paved
       NA       No alley access
       
```{r}
library(ggplot2)

ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
        geom_boxplot()
```
Alley might be important. Gravel homes have significantly lower price. Convert NAs to "NoAlley" to make more sense.

```{r}
train$Alley <- as.character(train$Alley)
train$Alley[which(is.na(train$Alley))] <- "NoAlley"
train$Alley <- factor(train$Alley)

#Also transform the test set
test$Alley <- as.character(test$Alley)
test$Alley[which(is.na(test$Alley))] <- "NoAlley"
test$Alley <- factor(test$Alley)
```

###FireplaceQu

Based on the data description:

FireplaceQu: Fireplace quality

       Ex       Excellent - Exceptional Masonry Fireplace
       Gd       Good - Masonry Fireplace in main level
       TA       Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement
       Fa       Fair - Prefabricated Fireplace in basement
       Po       Poor - Ben Franklin Stove
       NA       No Fireplace

```{r}

ggplot(data = train, aes(x = FireplaceQu, y = log10(SalePrice), fill= FireplaceQu))+
        geom_boxplot()

```

Let's look at through a simple linear model:

```{r}
par(mfrow=c(1,2))
hist(train$SalePrice)
hist(log10(train$SalePrice)) # Looks more gaussian



FirePlaceFit <- lm(log10(train$SalePrice) ~ FireplaceQu, data = train)
summary(FirePlaceFit)
par(mfrow=c(2,2))
plot(FirePlaceFit)
```
It appears that all levels of this feature also have significantly different mean Sales prices. Again, converting NAs to NoFireplace to be more informative.

```{r}
train$FireplaceQu <- as.character(train$FireplaceQu)
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "NoFireplace"
train$FireplaceQu <- factor(train$FireplaceQu)

#Also transform the test set
test$FireplaceQu <- as.character(test$FireplaceQu)
test$FireplaceQu[which(is.na(test$FireplaceQu))] <- "NoFireplace"
test$FireplaceQu <- factor(test$FireplaceQu)
```

###PoolQC

Based on the data description:

        PoolQC: Pool quality
               Ex       Excellent
               Gd       Good
               TA       Average/Typical
               Fa       Fair
               NA       No Pool
       
```{r}       
library(ggplot2)
ggplot(data = train, aes(x = PoolQC, y = log10(SalePrice), fill = PoolQC))+
        geom_boxplot()

```

Note that actually very few houses have pools. Houses with excellent pool condition have significantly higher sales prices. We will also keep this feature, converting NAs to "NoPool":

```{r}
train$PoolQC <- as.character(train$PoolQC)
train$PoolQC[which(is.na(train$PoolQC))] <- "NoPool"
train$PoolQC <- factor(train$PoolQC)

#Also transform the test set
test$PoolQC <- as.character(test$PoolQC)
test$PoolQC[which(is.na(test$PoolQC))] <- "NoPool"
test$PoolQC <- factor(test$PoolQC)
```

###Fence

Based on the data description:

        Fence: Fence quality
               GdPrv    Good Privacy
               MnPrv    Minimum Privacy
               GdWo     Good Wood
               MnWw     Minimum Wood/Wire
               NA       No Fence 
   
```{r}
library(ggplot2)
ggplot(data = train, aes(x = Fence, y = log10(SalePrice), fill = Fence))+
        geom_boxplot()

FenceFit <- lm(log10(train$SalePrice) ~ Fence, data = train)
summary(FenceFit)

```

Interestingly, this feature might have a complex relationship, if any at all, with the response variable. The effects of each of the level deemed as significant, so we will keep this feature at this point as well.

```{r}
train$Fence <- as.character(train$Fence)
train$Fence[which(is.na(train$Fence))] <- "NoFence"
train$Fence <- factor(train$Fence)

#Also transform the test set
test$Fence <- as.character(test$Fence)
test$Fence[which(is.na(test$Fence))] <- "NoFence"
test$Fence <- factor(test$Fence)
```

###MiscFeature

Based on the data description:

        MiscFeature: Miscellaneous feature not covered in other categories
               Elev     Elevator
               Gar2     2nd Garage (if not described in garage section)
               Othr     Other
               Shed     Shed (over 100 SF)
               TenC     Tennis Court
               NA       None
               
We will deal with the remaining missing value containing features in the next stages of our analysis.

```{r}
table(train$MiscFeature)
ggplot(data = train, aes(x = MiscFeature, y = log10(SalePrice), fill = MiscFeature))+
        geom_boxplot()
```
Again, very few houses have this features, and not sure if these qualities have a real impact on the house value amongst other measured features. Neverthless, I will maintain this feature as well. Just converting NAs to "NoMiscFeature" to be more clear:

```{r}
train$MiscFeature <- as.character(train$MiscFeature)
train$MiscFeature[which(is.na(train$MiscFeature))] <- "NoMiscFeature"
train$MiscFeature <- factor(train$MiscFeature)

#Also transform the test set
test$MiscFeature <- as.character(test$MiscFeature)
test$MiscFeature[which(is.na(test$MiscFeature))] <- "NoMiscFeature"
test$MiscFeature <- factor(test$MiscFeature)
```

#Developing expectations from data

Here I will explore the data further, in order to develope a better understanding about the relationships between features and the home prices.

## Continuous features: watching for strong predictors and potential collinearities 

Let's look at the continuous features in the data set:

```{r, fig.align='center'}
cont.var <-NULL
for(i in 1:ncol(train)){
        if(class(train[,i]) == "integer" | class(train[,i]) == "numeric"){
                cont.var <- c(cont.var,i)
        }
}
```

###Part1: transforming LotArea to make it more gaussian

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[1:10]], cex = 0.05, pch = 19, col = "navy")
```

- We need to remove ID, since this is just an identifier for the houses, has no predictive value as expected.
- OverallQual and OverallCond look strong predictors, but might have some collinearity.
- Yearbuilt and YearRemodAdd also look good predictors but most likely contain redundant information.
- For the LOT and SF features, it could be better to look at after log transformation, since the data looks skewed.

```{r,fig.width=6,fig.height=6}
par(mfrow=c(3,2))
hist(train$LotArea, breaks = 50, col = "navy");hist(log10(train$LotArea+1),breaks=50,col = "navy")
hist(train$MasVnrArea, breaks = 50,col = "navy");hist(log10(train$MasVnrArea+1),breaks=50,col = "navy")
hist(train$BsmtFinSF1, breaks = 50,col = "navy");hist(log10(train$BsmtFinSF1+1),breaks=50,col = "navy")
```

Indeed one of these features (LotArea) look more gaussian when it is log10 transformed. 

```{r,fig.width=6,fig.height=6}
pairs(log10(train$SalePrice) ~ log10(train$LotArea+1) + log10(train$MasVnrArea+1) + log10(train$BsmtFinSF1+1),cex = 0.1, pch = 19, col = "navy")
```

When transformed, lotArea seems to do a better job in explaining the variability in the SalePrice, other 2 variables however, have substantial 0 values and it is questionable how much this transformation might help.

Therefore, I will only transform the LotArea at this stage and leave the other features as they are, for feature selection steps.



###Part2: Area related features generally explain variation in the SalePrice

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[11:20]], cex = 0.05, pch = 19, col = "navy")
```

###Part3: Size of the garage and number of rooms are good predictors

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[22:31]], cex = 0.05, pch = 19, col = "navy")
```

Arguably, these features are all related to the area of the house.

###Part4: fairly weak predictors

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[32:38]], cex = 0.05, pch = 19, col = "navy")
```

Processing the identified features:
```{r}
# Remove ID variable

train.ID <- train$Id #Hold it in a new variable
train <- train[, -1]

library(dplyr)

train <- dplyr::mutate(train, log10.LotArea = log10(LotArea))
train <- dplyr::select(train, -LotArea)

# Also transform in the test set
test <- dplyr::mutate(test, log10.LotArea = log10(LotArea))
test <- dplyr::select(test, -LotArea)

```

## Categorical predictors: feature engineering to dummy variables

Now let's look at the categorical variables to see if certain ones are more powerful than others:

```{r}
cat.var <-NULL
for(i in 1:ncol(train)){
        if(class(train[,i]) == "factor"){
                cat.var <- c(cat.var,i)
        }
}
length(cat.var)
```

We have 43 categorical variables, some of them have only few levels. This is a challenge in many ways. It is not feasible to look each of them individually, therefore, we will first convert them to binary variables and try to use regularization or decision tree-based approaches to select and use most useful features.

```{r}
library(caret)
# Write a function that gets a data set and converts all factor variables to dummy variables

convert.to.dummy <- function(data.set){
        cat.var <-NULL
        temp.data <- data.frame(1:nrow(data.set))
           for(i in 1:ncol(data.set)){
                if(class(data.set[,i]) == "factor"){
                        cat.var <- c(cat.var,i)
                        factor.levels <- levels(data.set[,i])
                                for(j in seq_along(factor.levels)){
                                dummy.vector = ifelse(data.set[,i] == factor.levels[j],1,0)
                                dummy.vector <- data.frame(dummy.vector)
                                colnames(dummy.vector)[1] = paste(names((data.set)[i]),
                                                                  factor.levels[j],sep = ".")
                                temp.data <- cbind(temp.data,dummy.vector)
                                }
                }
           }
           #Remove the original categorical variables from data.set
           data.set <- data.set[,-cat.var]     
           #Add the dummy.variable set
           temp.data <- temp.data[,-1] # remove the unnecessary column
           data.set <- cbind(data.set,temp.data)
           
           return(data.set)     
}

# Process the training set
training.processed <- convert.to.dummy(train)

# Process the test set
test.processed <- convert.to.dummy(test)

# Note that not all the levels are present in the test set, therefore we need to make the features same.

 

```