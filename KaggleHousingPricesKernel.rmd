---
title: "Housing Prices"
author: "Ozan Aygun"
date: "11/26/2017"
output: 
   html_document:
        toc: true
        number_sections: true
        depth: 4
        theme: cerulean
        highlight: tango
        df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(results = "markup", fig.align = "center",
                      fig.width = 5, fig.height = 4,message=FALSE,warning=FALSE)
```


# Introduction and summary

# Loading the data


```{r}
train <- read.csv("train.csv"); test <- read.csv("test.csv")
```

# Summarizing the data
```{r}
summary(train)
```

The outcome variable SalePrice is a complete variable, but there are a number of predictors with many missing values. Median house price is $163,000.

```{r}
par(mfrow = c(1,2))
boxplot(train$SalePrice,main = "SalePrice")
boxplot(log10(train$SalePrice), main = "Log10(SalePrice)")
```

## Understanding the impact of missing data

Which features have more than 30% missing data?

```{r}
na.status <- is.na(train)
na.sum <- apply(na.status,2,sum)
names(na.sum) <- colnames(train)
mostly_missing <- which(na.sum > (0.3 * nrow(train)))
na.sum[mostly_missing]
```

5 variables have substantial amount of missing data. Do they have impact on the house prices?

###Alley

Based on the data description: Type of alley access to property

       Grvl     Gravel
       Pave     Paved
       NA       No alley access
       
```{r}
library(ggplot2)

ggplot(data = train, aes(x = Alley, y = log(SalePrice), fill= Alley))+
        geom_boxplot()
```
Alley might be important. Gravel homes have significantly lower price. Convert NAs to "NoAlley" to make more sense.

```{r}
train$Alley <- as.character(train$Alley)
train$Alley[which(is.na(train$Alley))] <- "NoAlley"
train$Alley <- factor(train$Alley)

#Also transform the test set
test$Alley <- as.character(test$Alley)
test$Alley[which(is.na(test$Alley))] <- "NoAlley"
test$Alley <- factor(test$Alley)
```

###FireplaceQu

Based on the data description:

FireplaceQu: Fireplace quality

       Ex       Excellent - Exceptional Masonry Fireplace
       Gd       Good - Masonry Fireplace in main level
       TA       Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement
       Fa       Fair - Prefabricated Fireplace in basement
       Po       Poor - Ben Franklin Stove
       NA       No Fireplace

```{r}

ggplot(data = train, aes(x = FireplaceQu, y = log10(SalePrice), fill= FireplaceQu))+
        geom_boxplot()

```

Let's look at through a simple linear model:

```{r}
par(mfrow=c(1,2))
hist(train$SalePrice)
hist(log10(train$SalePrice)) # Looks more gaussian



FirePlaceFit <- lm(log10(train$SalePrice) ~ FireplaceQu, data = train)
summary(FirePlaceFit)
par(mfrow=c(2,2))
plot(FirePlaceFit)
```
It appears that all levels of this feature also have significantly different mean Sales prices. Again, converting NAs to NoFireplace to be more informative.

```{r}
train$FireplaceQu <- as.character(train$FireplaceQu)
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "NoFireplace"
train$FireplaceQu <- factor(train$FireplaceQu)

#Also transform the test set
test$FireplaceQu <- as.character(test$FireplaceQu)
test$FireplaceQu[which(is.na(test$FireplaceQu))] <- "NoFireplace"
test$FireplaceQu <- factor(test$FireplaceQu)
```

###PoolQC

Based on the data description:

        PoolQC: Pool quality
               Ex       Excellent
               Gd       Good
               TA       Average/Typical
               Fa       Fair
               NA       No Pool
       
```{r}       
library(ggplot2)
ggplot(data = train, aes(x = PoolQC, y = log10(SalePrice), fill = PoolQC))+
        geom_boxplot()

```

Note that actually very few houses have pools. Houses with excellent pool condition have significantly higher sales prices. We will also keep this feature, converting NAs to "NoPool":

```{r}
train$PoolQC <- as.character(train$PoolQC)
train$PoolQC[which(is.na(train$PoolQC))] <- "NoPool"
train$PoolQC <- factor(train$PoolQC)

#Also transform the test set
test$PoolQC <- as.character(test$PoolQC)
test$PoolQC[which(is.na(test$PoolQC))] <- "NoPool"
test$PoolQC <- factor(test$PoolQC)
```

###Fence

Based on the data description:

        Fence: Fence quality
               GdPrv    Good Privacy
               MnPrv    Minimum Privacy
               GdWo     Good Wood
               MnWw     Minimum Wood/Wire
               NA       No Fence 
   
```{r}
library(ggplot2)
ggplot(data = train, aes(x = Fence, y = log10(SalePrice), fill = Fence))+
        geom_boxplot()

FenceFit <- lm(log10(train$SalePrice) ~ Fence, data = train)
summary(FenceFit)

```

Interestingly, this feature might have a complex relationship, if any at all, with the response variable. The effects of each of the level deemed as significant, so we will keep this feature at this point as well.

```{r}
train$Fence <- as.character(train$Fence)
train$Fence[which(is.na(train$Fence))] <- "NoFence"
train$Fence <- factor(train$Fence)

#Also transform the test set
test$Fence <- as.character(test$Fence)
test$Fence[which(is.na(test$Fence))] <- "NoFence"
test$Fence <- factor(test$Fence)
```

###MiscFeature

Based on the data description:

        MiscFeature: Miscellaneous feature not covered in other categories
               Elev     Elevator
               Gar2     2nd Garage (if not described in garage section)
               Othr     Other
               Shed     Shed (over 100 SF)
               TenC     Tennis Court
               NA       None
               
We will deal with the remaining missing value containing features in the next stages of our analysis.

```{r}
table(train$MiscFeature)
ggplot(data = train, aes(x = MiscFeature, y = log10(SalePrice), fill = MiscFeature))+
        geom_boxplot()
```
Again, very few houses have this features, and not sure if these qualities have a real impact on the house value amongst other measured features. Neverthless, I will maintain this feature as well. Just converting NAs to "NoMiscFeature" to be more clear:

```{r}
train$MiscFeature <- as.character(train$MiscFeature)
train$MiscFeature[which(is.na(train$MiscFeature))] <- "NoMiscFeature"
train$MiscFeature <- factor(train$MiscFeature)

#Also transform the test set
test$MiscFeature <- as.character(test$MiscFeature)
test$MiscFeature[which(is.na(test$MiscFeature))] <- "NoMiscFeature"
test$MiscFeature <- factor(test$MiscFeature)
```

#Developing expectations from data

Here I will explore the data further, in order to develope a better understanding about the relationships between features and the home prices.

## Continuous features: watching for strong predictors and potential collinearities 

Let's look at the continuous features in the data set:

```{r, fig.align='center'}
cont.var <-NULL
for(i in 1:ncol(train)){
        if(class(train[,i]) == "integer" | class(train[,i]) == "numeric"){
                cont.var <- c(cont.var,i)
        }
}
```

###Part1: transforming LotArea to make it more gaussian

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[1:10]], cex = 0.05, pch = 19, col = "navy")
```

- We need to remove ID, since this is just an identifier for the houses, has no predictive value as expected.
- OverallQual and OverallCond look strong predictors, but might have some collinearity.
- Yearbuilt and YearRemodAdd also look good predictors but most likely contain redundant information.
- For the LOT and SF features, it could be better to look at after log transformation, since the data looks skewed.

```{r,fig.width=6,fig.height=6}
par(mfrow=c(3,2))
hist(train$LotArea, breaks = 50, col = "navy");hist(log10(train$LotArea+1),breaks=50,col = "navy")
hist(train$MasVnrArea, breaks = 50,col = "navy");hist(log10(train$MasVnrArea+1),breaks=50,col = "navy")
hist(train$BsmtFinSF1, breaks = 50,col = "navy");hist(log10(train$BsmtFinSF1+1),breaks=50,col = "navy")
```

Indeed one of these features (LotArea) look more gaussian when it is log10 transformed. 

```{r,fig.width=6,fig.height=6}
pairs(log10(train$SalePrice) ~ log10(train$LotArea+1) + log10(train$MasVnrArea+1) + log10(train$BsmtFinSF1+1),cex = 0.1, pch = 19, col = "navy")
```

When transformed, lotArea seems to do a better job in explaining the variability in the SalePrice, other 2 variables however, have substantial 0 values and it is questionable how much this transformation might help.

Therefore, I will only transform the LotArea at this stage and leave the other features as they are, for feature selection steps.



###Part2: Area related features generally explain variation in the SalePrice

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[11:20]], cex = 0.05, pch = 19, col = "navy")
```

###Part3: Size of the garage and number of rooms are good predictors

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[22:31]], cex = 0.05, pch = 19, col = "navy")
```

Arguably, these features are all related to the overall area of the house.

###Part4: fairly weak predictors

```{r,fig.width=12,fig.height=12}
pairs(log10(train$SalePrice) ~ ., data = train[,cont.var[32:38]], cex = 0.05, pch = 19, col = "navy")
```

Processing the identified features:
```{r}
# Remove ID variable

train.ID <- train$Id #Hold it in a new variable
train <- train[, -1]

library(dplyr)

train <- dplyr::mutate(train, log10.LotArea = log10(LotArea))
train <- dplyr::select(train, -LotArea)

# Also transform in the test set
test <- dplyr::mutate(test, log10.LotArea = log10(LotArea))
test <- dplyr::select(test, -LotArea)

```

## Categorical predictors: feature engineering to create dummy variables

Now let's look at the categorical variables to see if certain ones are more powerful than others:

```{r}
cat.var <-NULL
for(i in 1:ncol(train)){
        if(class(train[,i]) == "factor"){
                cat.var <- c(cat.var,i)
        }
}
length(cat.var)
```

We have 43 categorical variables, some of them have only few levels. This is a challenge in many ways. It is not feasible to look each of them individually, therefore, we will first convert them to binary variables and try to use regularization or decision tree-based approaches to select and use most useful features.

```{r}
# Write a function that gets a data set and converts all factor variables to dummy variables
convert.to.dummy <- function(data.set){
        cat.var <-NULL
        temp.data <- data.frame(1:nrow(data.set))
           for(i in 1:ncol(data.set)){
                if(class(data.set[,i]) == "factor"){
                        cat.var <- c(cat.var,i)
                        factor.levels <- levels(data.set[,i]) # Try to find a way to classify NA's as "NO" otherwise they generate problem downstream
                                # First check if there is any 'NA-level'
                                if(any(is.na(data.set[,i]))){
                                        dummy.vector = ifelse(is.na(data.set[,i]),1,0)
                                        dummy.vector <- data.frame(dummy.vector)
                                        colnames(dummy.vector)[1] = paste("NO",names((data.set)[i]),sep = ".")
                                        temp.data <- cbind(temp.data,dummy.vector)
                                }
                        
                                for(j in seq_along(factor.levels)){ # Then deal with normal factor levels
                                dummy.vector = ifelse(data.set[,i] == factor.levels[j],1,0)
                                
                                #Since we already dealt with NAs above
                                if(any(is.na(dummy.vector))){dummy.vector[is.na(dummy.vector)] <- 0} 
                                
                                dummy.vector <- data.frame(dummy.vector)
                                colnames(dummy.vector)[1] = paste(names((data.set)[i]),
                                                                  factor.levels[j],sep = ".")
                                temp.data <- cbind(temp.data,dummy.vector)
                                }
                }
           }
           #Remove the original categorical variables from data.set
           data.set <- data.set[,-cat.var]     
           #Add the dummy.variable set
           temp.data <- temp.data[,-1] # remove the unnecessary column
           data.set <- cbind(data.set,temp.data)
           
           return(data.set)     
}

# Keep test.IDs aside
test.ID <- test$Id

# Process the training set
training.processed <- convert.to.dummy(train)

# Process the test set
test.processed <- convert.to.dummy(test)

# Note that not all the levels are present in the test set, therefore we need to make the features same.
training.processed.SalePrice <- training.processed$SalePrice #Keep the original outcome variable 

consensus.features1 <- which(colnames(training.processed) %in% colnames(test.processed))
training.processed <- training.processed[,consensus.features1] #Keep the same features as in the test set

consensus.features2 <- which(colnames(test.processed) %in% colnames(training.processed))
test.processed <- test.processed[,consensus.features2] #Keep the same features as in the training set

identical(colnames(test.processed), colnames(training.processed))
# Same features in both sets!

training.processed <- data.frame(Log.SalePrice = log(training.processed.SalePrice),training.processed) # Add the Log of the response variable
test.processed <- data.frame(ID = test.ID,test.processed) # Add the ID variable

```

## Missing value imputation

We still have missing values in some of our features, which will be a problem down the road. 

```{r}

apply(is.na(training.processed), 2, sum)[apply(is.na(training.processed), 2, sum) != 0]

```
```{r}

apply(is.na(test.processed), 2, sum)[apply(is.na(test.processed), 2, sum) != 0]

```


##GarageYrBlt

We have 81 missing cases in our training set, these are the houses actually they don't have a garage.

```{r}
plot(y =training.processed$Log.SalePrice, x =training.processed$GarageYrBlt, pch = 19, col = "navy", cex = 0.5)
```

It looks like this feature has some relationship with the outcome, as also we can generally expect that newer houses are going to be more expensive. Therefore, we should make an attempt to impute the values missing from this feature.

```{r}
plot(training.processed$YearBuilt, training.processed$GarageYrBlt, pch = 19, col = "navy", cex = 0.5)
```

As we expected, for most of the houses the year that garage was built is the same as the year the house was built. Therefore, for imputation purpose it is sensible to use the YearBuilt feature to fill these issing values in the GarageYrBuilt feature. While this is not consistent with the fact that these houses do not contain any garage, since we captured that in the categorical features we engineered, at this point imputing the years seems to be more appropriate than removing these data points alltogether:

```{r}
#Impute in the training set
training.processed$GarageYrBlt[is.na(training.processed$GarageYrBlt)] <- training.processed$YearBuilt[is.na(training.processed$GarageYrBlt)]


#Impute in the test set
test.processed$GarageYrBlt[is.na(test.processed$GarageYrBlt)] <- test.processed$YearBuilt[is.na(test.processed$GarageYrBlt)]

```

##LotFrontage

Let's look at this feature more closely:

```{r}
plot(x = training.processed$LotFrontage, y = training.processed$Log.SalePrice, cex = 0.5, col = "navy" , pch = 19)
abline(lm(training.processed$Log.SalePrice ~ training.processed$LotFrontage))
```

Again this feature seems to have some degree of relationship with the response. We suspect that this feature has relationship with other features that relate to the Lot or the general area of the house:

```{r}
cor(training.processed$LotFrontage,training.processed$log10.LotArea, use = "complete.obs")
cor(training.processed$LotFrontage,training.processed$GarageArea, use = "complete.obs")
cor(training.processed$LotFrontage,training.processed$MasVnrArea, use = "complete.obs")

# Note that correlation gets better when log10 transformed:
cor(log10(training.processed$LotFrontage), training.processed$log10.LotArea, use = "complete.obs")

summary(lm(training.processed$Log.SalePrice ~ log10(training.processed$LotFrontage) + training.processed$log10.LotArea ))

plot(y = log10(training.processed$LotFrontage), x = training.processed$log10.LotArea, cex = 0.5, col = "navy" , pch = 19)
abline(lm(log10(training.processed$LotFrontage) ~ training.processed$log10.LotArea))
```
It looks like this feature pretty much collinear with the LotArea feature, especially when log10 transformed. Therefore, we will remove this feature from both training and test data sets.

```{r}
training.processed <- dplyr::select(training.processed, -LotFrontage)
test.processed <- dplyr::select(test.processed, -LotFrontage)
```

##MasVnrArea: Masonry veneer area in square feet

Looking at more closely to this feature:

```{r}
plot(x = training.processed$MasVnrArea, y = training.processed$Log.SalePrice,cex = 0.5, col = "navy" , pch = 19)

```
Note that a large number of these values are zero, which probably reflects the houses with no masonry veener. What happens if we put these aside and look at the rest of the values:

```{r}
plot(x = log10(training.processed$MasVnrArea[training.processed$MasVnrArea != 0]), y = training.processed$Log.SalePrice[training.processed$MasVnrArea != 0],cex = 0.5, col = "navy" , pch = 19)
```

There might be 'some' relationship with the response, if not too strong. It is also a little confusing since the variable MasVnrType only has these 8 values as "none", but linked the large amount of MasVnrAre == 0 data to certain MasVnrTypes:


```{r}
qplot(data = training.processed,x = log10(training.processed$MasVnrArea), y = training.processed$Log.SalePrice, col = train$MasVnrType)
     
```

It is more clear now. The "none" group indeed represent the MasVnrArea = 0, NAs however appear as missing values in both cases.

```{r}
ggplot(data = training.processed,aes(x = train$MasVnrType, y = training.processed$Log.SalePrice, fill = train$MasVnrType))+geom_boxplot()
```
We note that the SalePrice of the missing cases in the training set is closest to the SalePrice of the houses with Stone Masonry Veener type. It is likely that the area of the veener in some of these houses were not measured, perhaps it is more difficult to do so. This is our assumption.

Based on this assumption, we will merge the NA cases with the Stone group and remove the MasVnrArea feature in both training and test data sets:

```{r}
training.processed$MasVnrType.None[training.processed$NO.MasVnrType == 1] <- 1
training.processed <- dplyr::select(training.processed, -NO.MasVnrType, -MasVnrArea)

test.processed$MasVnrType.None[test.processed$NO.MasVnrType == 1] <- 1
test.processed <- dplyr::select(test.processed, -NO.MasVnrType, -MasVnrArea)
```


## Missing data remaining in the test set

We still need to deal with few missing data points we have in the test set. Since we need to make SalePrice predictions for each of these cases, we need to assess how we can impute these values at this point:

```{r}
apply(is.na(test.processed), 2, sum)[apply(is.na(test.processed), 2, sum) != 0]
```

Some features regarding Basement and Garage have missing values:

```{r}
test.processed[which(apply(is.na(test.processed), 1, sum) != 0),which(apply(is.na(test.processed), 2, sum) != 0 )]
```

Fortunately, the missing values are only restricted to 3 house records. It is likely these unusual houses are either lacking basement or garage. Since all these features are numeric, we will simply place zero instead of NA's for these values. This is of course based on our assumption.

```{r}
for (i in 1:ncol(test.processed)) {
        if(any(is.na(test.processed[,i]))){
                test.processed[is.na(test.processed[,i]),i] <- 0
        }
}
```


This completes missing value imputation as no missing values left in either training and test data sets.

#Predictive modeling using the training set

##Using shrinkage methods (regularization)

It would be  interesting to perform a linear model selection and see how well it might perform with the new training set we obtained.

### Fitting a lasso regression model to data

```{r}

library(glmnet)

# We create a matrix of predictors
x.training = model.matrix(Log.SalePrice ~ . -1, data = training.processed)
x.testing = model.matrix( ~ . -1, data = test.processed)

y = training.processed$Log.SalePrice

# Fit lasso regression to training data
fit.lasso <- glmnet(x.training,y, family = "gaussian")
plot(fit.lasso, xvar = "lambda", label = TRUE)
plot(fit.lasso, xvar = "dev", label = TRUE)

```


Lasso is a nice way of reducing the features while still trying to explain the variance and therefore trying to win the bias-variance trade-off. In this case we notice that only 3 features are able to explain 60% of the deviance, while adding 13 features is able to explain 80% of it. It is more feasible to use the restricted model with the few features and their shrunken coefficients.

###Choosing the optimal lambda by using cross-validation

It is best to choose the optimal lambda using cross-validation, with the aim of minimizing **mean squared error:**:

```{r}
#cv.lasso will be the list containing the coefficients and information of our optimal model fit using cross-validation (10 fold by default)
cv.lasso <- cv.glmnet(x.training,y, family = "gaussian")
plot(cv.lasso)

```

Notice that what we are doing here is actually computationally intense.

1. We get very fine grids of lambda values.
2. Using each lambda value, we perform 10-fold cross validation:

- We fit a model using the Sum of Squares convex optimization and obtain cross-validated error (average of 10 model fits)

3. We continue steps 1 and 2 for all fine grids of lambda values in the range.

The two vertical lines are produced in the plot, the left one marks the model with min error and the middle one shows a little more restricted model 1 standard error away from the minimum error.

**When choosing the optimal model, we might prefer to get the more restricted model that is indicated by the second vertical line, which is default in glmnet package, due to the 1 standard error conversion.**

The final (optimal) model coefficients we obtained by cross-validation and the non-zero features remaining:

```{r}
coef(cv.lasso)
```

Note that our final model includes ~30 features with non-zero coefficients. We therefore significantly reduced the size of the data set by selecting features through lasso regression. The resulting model explains more than 80% of the deviance in the outcome, which is considerable good for the training set.

We also note that some of the dummy variables remained in the model, while majority of them removed. It seems to be good idea to keep all levels as seperate dummy variables until the regularization, since we don't know which ones may be important in the final model context.

###Making predictions using the best lasso model fit

Let's first start with the training set:

```{r}
pred <- predict(cv.lasso, newx = x.training)

#Calculating the root mean squared error
rmse.training = sqrt(mean((training.processed$Log.SalePrice - pred)^2))
rmse.training

#Pearson correlation:
cor.training = cor(training.processed$Log.SalePrice,pred)

plot(x = pred, y = training.processed$Log.SalePrice, cex = 0.5, col = "navy", pch = 19)
text(x = max(pred), y = min(training.processed$Log.SalePrice)+0.3,
     labels = paste0("RMSE: ",rmse.training))
```

Therefore, the RMSE we obtained from the training set is ~15%. 

Next we will try to use test set to estimate the house prices using our model.